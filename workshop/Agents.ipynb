{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc0af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Values taken from game\n",
    "\n",
    "inner_radius = 5\n",
    "outer_radius = 15\n",
    "rider_height = 4\n",
    "rider_offset_y = 0.6\n",
    "\n",
    "wheel_radius = 1.1\n",
    "wheel_mass = 5\n",
    "wheel_inertia = 70\n",
    "gravity = 9.81\n",
    "friction = 0.9\n",
    "\n",
    "pedaling_acceleration = 3 # radians\n",
    "lean_speed = math.radians(200)\n",
    "\n",
    "delta_time = 1/60 # 60 FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1b926ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct classes/helpers for 2d equivalent of the game,\n",
    "# with numerically equivalent dynamics.\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "def lerp(a, b, t):\n",
    "    return t*b + (1-t)*a\n",
    "\n",
    "def invlerp(a, b, c):\n",
    "    return (c-a)/(b-a)\n",
    "\n",
    "def absolute_angle(x, y):\n",
    "    angle = math.atan2(y, x)\n",
    "    if angle < 0:\n",
    "        angle = math.pi + abs(angle + math.pi)\n",
    "    return angle\n",
    "\n",
    "class Loop:\n",
    "    def __init__(self, inner_radius, outer_radius):\n",
    "        self.inner_radius = inner_radius\n",
    "        self.outer_radius = outer_radius\n",
    "        \n",
    "    def on_track(self, position):\n",
    "        r = math.hypot(position[0], position[1])\n",
    "        return r > self.inner_radius and r < self.outer_radius\n",
    "        \n",
    "    def generate_starting_position(self):\n",
    "        r = random.randrange(self.inner_radius, self.outer_radius)\n",
    "        return [r, 0]\n",
    "        \n",
    "    def draw_canvas(self, canvas):\n",
    "        canvas.save()\n",
    "        \n",
    "        canvas.begin_path()\n",
    "        canvas.fill_style = '#aaa'\n",
    "        canvas.arc(0, 0, self.outer_radius, 0, 2 * math.pi, False)\n",
    "        canvas.arc(0, 0, self.inner_radius, 0, 2 * math.pi, True)\n",
    "        canvas.fill()\n",
    "        \n",
    "        canvas.begin_path()\n",
    "        canvas.stroke_style = '#000'\n",
    "        canvas.line_width = 0.1\n",
    "        canvas.move_to(self.inner_radius, 0)\n",
    "        canvas.line_to(self.outer_radius, 0)\n",
    "        canvas.stroke()\n",
    "        \n",
    "        canvas.restore()\n",
    "\n",
    "class Unicycle:\n",
    "    def __init__(self, position=[0, 0], track=None):\n",
    "        self.track = track\n",
    "        self.position = position\n",
    "        self.heading_angle = math.pi/2\n",
    "        self.heading = [math.cos(self.heading_angle), math.sin(self.heading_angle)]\n",
    "        \n",
    "        # Unicycle state (radians)\n",
    "        self.rider_lean_angle       = 0 # Lean of rider wrt front of unicycle\n",
    "        self.unicycle_roll_angle    = 0 # Lean of unicycle wrt track\n",
    "        self.unicycle_roll_momentum = 0 # Angular momentum of unicycle\n",
    "        self.wheel_angular_momentum = 0\n",
    "        \n",
    "        self.failed = False\n",
    "        \n",
    "    def clone(self, to=None):\n",
    "        # TODO: Be able to evaluate actions w/o stepping unicycle\n",
    "\n",
    "        if to is None:\n",
    "            to = Unicycle(self.position, self.track)\n",
    "\n",
    "        to.position[0] = self.position[0]\n",
    "        to.position[1] = self.position[1]\n",
    "        to.track = self.track        \n",
    "\n",
    "        to.heading_angle = self.heading_angle\n",
    "        to.heading = self.heading\n",
    "\n",
    "        to.rider_lean_angle       = self.rider_lean_angle # Lean of rider wrt front of unicycle\n",
    "        to.unicycle_roll_angle    = self.unicycle_roll_angle # Lean of unicycle wrt track\n",
    "        to.unicycle_roll_momentum = self.unicycle_roll_momentum # Angular momentum of unicycle\n",
    "        to.wheel_angular_momentum = self.wheel_angular_momentum\n",
    "\n",
    "        return to\n",
    "        \n",
    "    def draw_canvas(self, canvas):\n",
    "        canvas.save()\n",
    "        \n",
    "        length = 2\n",
    "        width = 1\n",
    "        \n",
    "        canvas.translate(self.position[0], self.position[1])\n",
    "        canvas.rotate(self.heading_angle)\n",
    "\n",
    "        # Osculating circle\n",
    "        if abs(self.unicycle_roll_angle) > 1e-3:\n",
    "            canvas.begin_path()\n",
    "            denom = lerp(0, 1, invlerp(0, math.pi/2, abs(self.unicycle_roll_angle)))\n",
    "            k = math.inf if abs(denom) < 1e-6 else 1/denom\n",
    "            canvas.begin_path()\n",
    "            canvas.line_width = 0.3\n",
    "            canvas.stroke_style = '#ff0'\n",
    "            canvas.arc(0, math.copysign(k, self.unicycle_roll_angle), k, 0, 2 * math.pi)\n",
    "            canvas.stroke()\n",
    "            \n",
    "        # \"Wheel\"/body\n",
    "        canvas.fill_style = '#f00' if self.failed else '#0f0'\n",
    "        canvas.fill_rect(-length/2, -width/2, length, width) # Align w/ heading, so on x-axis\n",
    "        \n",
    "        # Heading arrow\n",
    "        canvas.begin_path()\n",
    "        canvas.stroke_style = '#ff0'\n",
    "        canvas.line_width = 0.3\n",
    "        canvas.move_to(length/2, 0)\n",
    "        canvas.line_to(length/2 + 2, 0)\n",
    "        canvas.stroke()\n",
    "        \n",
    "        # Wheel roll helper arrow\n",
    "        canvas.begin_path()\n",
    "        canvas.stroke_style = '#00f' if self.unicycle_roll_angle > 0 else '#f00'\n",
    "        canvas.move_to(0, math.copysign(width/2, self.unicycle_roll_angle))\n",
    "        canvas.line_to(0, (math.copysign(width/2, self.unicycle_roll_angle) + math.sin(self.unicycle_roll_angle) * 1.5))\n",
    "        canvas.stroke()\n",
    "        \n",
    "        canvas.restore()\n",
    "        \n",
    "    def act(self, action, delta_time):\n",
    "        # Take in action (maps to keyboard presses in game),\n",
    "        # update rider lean angle, wheel pitch momentum\n",
    "\n",
    "        # type UnicycleInput = {\n",
    "        #    leaning: -1 | 0 | 1,\n",
    "        #    pedaling: -2 | -1 | 0 | 1 | 2,\n",
    "        # }\n",
    "        \n",
    "        # action: [leaning, pedaling]\n",
    "        \n",
    "        # Assume that leaning/pedaling in action are already discretized\n",
    "        leaning = action[0]\n",
    "        pedaling = action[1]\n",
    "        \n",
    "        self.wheel_angular_momentum += pedaling * pedaling_acceleration * delta_time\n",
    "\n",
    "        dtheta = leaning * lean_speed * delta_time\n",
    "        self.rider_lean_angle -= dtheta\n",
    "\n",
    "    def update(self, delta_time):\n",
    "        # Perform physics update step, according to delta time\n",
    "        \n",
    "        # Update roll from leaning\n",
    "        r_x = math.sin(self.unicycle_roll_angle) * (wheel_radius * 2) \\\n",
    "            + math.sin(self.unicycle_roll_angle + self.rider_lean_angle) * (rider_height/2 + rider_offset_y)\n",
    "        torque = r_x * gravity * wheel_mass\n",
    "\n",
    "        self.unicycle_roll_momentum += torque/wheel_inertia * delta_time\n",
    "\n",
    "        self.unicycle_roll_angle += self.unicycle_roll_momentum * delta_time\n",
    "        self.unicycle_roll_angle = np.clip(self.unicycle_roll_angle, -math.pi/2, math.pi/2)\n",
    "\n",
    "        # Update wheel rotation\n",
    "        dx = self.wheel_angular_momentum * wheel_radius * delta_time\n",
    "        self.wheel_angular_momentum -= dx * friction\n",
    "\n",
    "        # Update heading according to curvature\n",
    "        denom = lerp(0, 1, invlerp(0, math.pi/2, abs(self.unicycle_roll_angle)))\n",
    "        k = math.inf if abs(denom) < 1e-3 else 1/denom\n",
    "\n",
    "        dtheta = self.wheel_angular_momentum/k * delta_time\n",
    "        self.heading_angle -= math.copysign(dtheta, -self.unicycle_roll_angle)\n",
    "\n",
    "        self.heading[0] = math.cos(self.heading_angle)\n",
    "        self.heading[1] = math.sin(self.heading_angle)\n",
    "        \n",
    "        # Move according to heading\n",
    "        self.position[0] += self.heading[0] * dx\n",
    "        self.position[1] += self.heading[1] * dx\n",
    "        \n",
    "        # Update failed or not, used to terminate episode\n",
    "        if not self.failed:\n",
    "            self.failed = not self.track.on_track(self.position) \\\n",
    "                or abs(self.unicycle_roll_angle) > math.pi/2 - 1e-1\n",
    "        \n",
    "    # Steps through timestep given action, returns reward\n",
    "    def step(self, action, delta_time):\n",
    "        starting_abs_angle = absolute_angle(self.position[0], self.position[1])\n",
    "\n",
    "        self.act(action, delta_time)\n",
    "        self.update(delta_time)\n",
    "\n",
    "        ending_abs_angle = absolute_angle(self.position[0], self.position[1])\n",
    "\n",
    "        # Then must have driven backwards from finish line, which ends the episode\n",
    "        if ending_abs_angle - starting_abs_angle > abs(math.pi):\n",
    "            self.failed = True\n",
    "            return 0\n",
    "        \n",
    "        # Reward is progress made around circle\n",
    "        progress = ending_abs_angle - starting_abs_angle\n",
    "\n",
    "        return progress\n",
    "            \n",
    "            \n",
    "# Abstract all the drawing logic for\n",
    "# visualizing episodes to this function,\n",
    "# to then be used in following cells.\n",
    "def draw(canvas, entities):\n",
    "    screen_width, screen_height = canvas.width, canvas.height\n",
    "    \n",
    "    world_width = 40\n",
    "    world_height = screen_height/screen_width * world_width\n",
    "    \n",
    "    canvas.clear()\n",
    "\n",
    "    canvas.save()\n",
    "\n",
    "    canvas.scale(screen_width/world_width, -screen_height/world_height)\n",
    "    canvas.translate(world_width/2, -world_height/2)\n",
    "\n",
    "    for entity in entities:\n",
    "        entity.draw_canvas(canvas)\n",
    "\n",
    "    canvas.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a16196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matrix of all possible actions, an action being both an input for leaning and pedaling.\n",
    "# To simplify, we'll ignore that the player can hold shift to pedal twice as fast, and just take all\n",
    "# combinations of leaning forward, backward, or not at all (-1, 0, 1) and of pedaling forward, backward or not at all (-1, 0, 1).\n",
    "# This results in 3*3=9 combinations, each with two entries (leaning/pedaling), making a 9x2 matrix. \n",
    "possible_actions = np.array([[[p, l] for p in [-1, 0, 1]] for l in [-1, 0, 1]]).reshape(9, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "244cce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state is a 10x1 feature vector (9 features + bias)\n",
    "# action is a 2x1 vector\n",
    "    # type UnicycleInput = {\n",
    "    #    leaning: -1 | 0 | 1,\n",
    "    #    pedaling: -2 | -1 | 0 | 1 | 2,\n",
    "    # }\n",
    "\n",
    "def get_state(unicycle):\n",
    "    # [r, cos_theta, sin_theta, cos_heading, sin_heading, rider_lean, lean, lean_momentum, wheel_momentum, 1]\n",
    "    \n",
    "    # Collect values from unicycle and normalize them.\n",
    "    # Use Cartesian coordinates to represent angles, to remove discontinuity.\n",
    "    x, y = unicycle.position\n",
    "    r = math.hypot(x, y)\n",
    "    cos_theta, sin_theta = x/r, y/r\n",
    "    cos_heading = math.cos(unicycle.heading_angle)\n",
    "    sin_heading = math.sin(unicycle.heading_angle)\n",
    "    rider_lean = unicycle.rider_lean_angle/(math.pi/2)\n",
    "    lean = unicycle.unicycle_roll_angle/(math.pi/2)\n",
    "    wheel_momentum = unicycle.wheel_angular_momentum/6 # Max asymptotic wheel omega\n",
    "    lean_momentum = unicycle.unicycle_roll_momentum # Angular vel, so no real meaning for unit\n",
    "    \n",
    "    # Flip sign of lean to be consistent? (I didn't bother figuring this out)\n",
    "    lean *= -1\n",
    "    lean_momentum *= -1\n",
    "    \n",
    "    # Add 1 as a last entry as a bias term an agent can incorporate\n",
    "    return np.array([r, cos_theta, sin_theta, cos_heading, sin_heading, rider_lean, lean, wheel_momentum, lean_momentum, 1])\n",
    "\n",
    "u = Unicycle()\n",
    "\n",
    "def featurize_state_action(unicycle, action, delta_time):\n",
    "    # Featurize state but incorporate action through\n",
    "    # cheating by updating the environment. However, this incorporates\n",
    "    # both the state and the action into a unique feature vector.\n",
    "    # This should differentiate the features for each action for a given state,\n",
    "    # making it more possible for the agent to discern between them.\n",
    "\n",
    "    unicycle.clone(to=u)\n",
    "\n",
    "    u.step(action, delta_time)\n",
    "    state = get_state(unicycle)\n",
    "    \n",
    "    return state\n",
    "\n",
    "def score(state):\n",
    "    # Naive reward: (absolute) angle of ending point wrt finish line.\n",
    "    # Should maybe encourage the agent to move backwards?\n",
    "    x, y = state[1], state[2] # Use cos_theta, sin_theta\n",
    "    angle = math.atan2(y, x)\n",
    "    if angle < 0:\n",
    "        angle = math.pi + abs(angle + math.pi)\n",
    "    return angle\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def policy(self, state):\n",
    "        r, cos_theta, sin_theta, cos_heading, sin_heading, rider_lean, lean, wheel_momentum, lean_momentum = state\n",
    "\n",
    "#         # [leaning, pedaling]\n",
    "#         p = softmax([abs(min(0, lean)), 1e-3, max(0, lean)])\n",
    "#         leaning = -1\n",
    "#         if lean_momentum > 0.05:\n",
    "#             leaning = -1\n",
    "#         elif lean_momentum < -0.05:\n",
    "#             leaning = 1\n",
    "            \n",
    "#         # return [np.random.choice([-1, 0, 1], p=p), 1]\n",
    "#         return [leaning, 1]\n",
    "\n",
    "# Agent that acts greedily according to a linear action-value (Q) function\n",
    "# defined by a weight matrix `w` (the value of an action is a unique linear\n",
    "# combination of all features). \n",
    "class GreedyLinearAgent:\n",
    "    def __init__(self, w, unicycle, delta_time):\n",
    "        # 9x10 weight matrix: separate weight vector for each action.\n",
    "        self.w = w\n",
    "        self.delta_time = delta_time\n",
    "        self.unicycle = unicycle\n",
    "    \n",
    "    def policy(self, state):\n",
    "        featurized_action_value_pairs = np.array([\n",
    "            featurize_state_action(self.unicycle, action, self.delta_time)\n",
    "             for action in possible_actions\n",
    "        ])\n",
    "        action_values = np.sum(np.multiply(self.w, featurized_action_value_pairs), axis=1)\n",
    "        \n",
    "        action = possible_actions[np.argmax(action_values)]\n",
    "            \n",
    "        return action\n",
    "\n",
    "action_indices = np.arange(len(possible_actions))\n",
    "\n",
    "class EpsGreedyLinearAgent:\n",
    "    def __init__(self, w, unicycle, delta_time, eps):\n",
    "        self.w = w\n",
    "        self.unicycle = unicycle\n",
    "        self.eps = eps\n",
    "        self.gla = GreedyLinearAgent(w, unicycle, delta_time)\n",
    "        self.delta_time = delta_time\n",
    "        \n",
    "    def replay(self, state, action, ret):\n",
    "        # TODO: Clean this up\n",
    "        action_index = 0\n",
    "        for i, a in enumerate(possible_actions):\n",
    "            if abs(a[0]-action[0]) < 1e-3 and abs(a[1]-action[1]) < 1e-3:\n",
    "                action_index = i\n",
    "                break\n",
    "\n",
    "        target = ret\n",
    "        featurized = featurize_state_action(self.unicycle, action, self.delta_time)\n",
    "        current = self.w[action_index] @ featurized\n",
    "        alpha = 0.5\n",
    "        self.w[action_index] -= alpha * (target - current) * featurized\n",
    "        \n",
    "    def policy(self, state):\n",
    "        self.gla.w = self.w\n",
    "        greedy_action = self.gla.policy(state)\n",
    "\n",
    "        if np.random.randn() < (1 - self.eps):\n",
    "            return greedy_action\n",
    "\n",
    "        random_action_index = np.random.choice(action_indices)\n",
    "        random_action = possible_actions[random_action_index]\n",
    "\n",
    "        return random_action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0b154aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1883036bfc684fba98d1ca0d63934985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(height=300, width=400)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# Episode rollout loop example.\n",
    "# Each iteration, featurize the state of the unicycle,\n",
    "# pass it to the policy (in this case something dumb),\n",
    "# step the unicycle one timestep through that action,\n",
    "# then draw to the canvas to visualize the policy.\n",
    "\n",
    "from time import sleep\n",
    "from ipycanvas import Canvas, hold_canvas\n",
    "\n",
    "canvas = Canvas(width=400, height=300)\n",
    "display(canvas)\n",
    "\n",
    "track = Loop(inner_radius, outer_radius)\n",
    "\n",
    "agent = Agent()\n",
    "unicycle = Unicycle(position=track.generate_starting_position(), track=track)\n",
    "\n",
    "def policy(state):\n",
    "    leaning = np.clip(math.floor(np.sum(state)/3) - 1, -1, 1)\n",
    "    pedaling = 2 - leaning\n",
    "\n",
    "    return [leaning, pedaling]\n",
    "\n",
    "n = 0\n",
    "while True:\n",
    "    with hold_canvas(): # wrap in hold_canvas() for performant canvas updates\n",
    "        if unicycle.failed: # .failed becomes true whenever unicycle gets off-track or tips over\n",
    "            break\n",
    "\n",
    "        state = get_state(unicycle)\n",
    "        action = policy(state)\n",
    "        unicycle.step([-1, 1], delta_time)\n",
    "        \n",
    "        n += 1\n",
    "\n",
    "        draw(canvas, [track, unicycle])\n",
    "\n",
    "        if n == 50:\n",
    "            break\n",
    "            \n",
    "        # Optional\n",
    "        sleep(delta_time)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad0958f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loop\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m state \u001b[39m=\u001b[39m get_state(unicycle)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mpolicy(state)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m reward \u001b[39m=\u001b[39m unicycle\u001b[39m.\u001b[39;49mstep(action, delta_time)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m delta_position \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mhypot(unicycle\u001b[39m.\u001b[39mposition[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m old_x, unicycle\u001b[39m.\u001b[39mposition[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m old_y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m return_ \u001b[39m=\u001b[39m discount_factor \u001b[39m*\u001b[39m return_ \u001b[39m+\u001b[39m reward\n",
      "\u001b[1;32m/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m starting_abs_angle \u001b[39m=\u001b[39m absolute_angle(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition[\u001b[39m1\u001b[39m])\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(action, delta_time)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=187'>188</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(delta_time)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m ending_abs_angle \u001b[39m=\u001b[39m absolute_angle(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition[\u001b[39m1\u001b[39m])\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m \u001b[39m# Then must have driven backwards from finish line, which ends the episode\u001b[39;00m\n",
      "\u001b[1;32m/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39municycle_roll_momentum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torque\u001b[39m/\u001b[39mwheel_inertia \u001b[39m*\u001b[39m delta_time\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39municycle_roll_angle \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39municycle_roll_momentum \u001b[39m*\u001b[39m delta_time\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39municycle_roll_angle \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mclip(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49municycle_roll_angle, \u001b[39m-\u001b[39;49mmath\u001b[39m.\u001b[39;49mpi\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m, math\u001b[39m.\u001b[39;49mpi\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m \u001b[39m# Update wheel rotation\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/profsucrose/Documents/unicycle-rl-workshop/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m dx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwheel_angular_momentum \u001b[39m*\u001b[39m wheel_radius \u001b[39m*\u001b[39m delta_time\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2169\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2100\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_clip_dispatcher)\n\u001b[1;32m   2101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclip\u001b[39m(a, a_min, a_max, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   2102\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m \u001b[39m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2167\u001b[0m \n\u001b[1;32m   2168\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2169\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mclip\u001b[39;49m\u001b[39m'\u001b[39;49m, a_min, a_max, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/_methods.py:99\u001b[0m, in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m um\u001b[39m.\u001b[39mmaximum(a, \u001b[39mmin\u001b[39m, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     98\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     \u001b[39mreturn\u001b[39;00m um\u001b[39m.\u001b[39;49mclip(a, \u001b[39mmin\u001b[39;49m, \u001b[39mmax\u001b[39;49m, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# https://proceedings.neurips.cc/paper_files/paper/2013/file/7504adad8bb96320eb3afdd4df6e1f60-Paper.pdf\n",
    "# Use action-value function (Q) defined as a unique linear combination\n",
    "# of the feature vector for each action. Weights are passed in, where the CEM\n",
    "# algorithm is done to update the family of agents each iteration.\n",
    "\n",
    "from time import sleep\n",
    "from ipycanvas import Canvas, hold_canvas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# canvas = Canvas(width=400, height=300)\n",
    "# display(canvas)\n",
    "\n",
    "track = Loop(inner_radius, outer_radius)\n",
    "\n",
    "# Shape of input/output\n",
    "n_actions = 9\n",
    "n_state_features = 10\n",
    "\n",
    "# Hyperparameters for initial (meta-)weights generation\n",
    "start_means_mean = 0\n",
    "start_means_stdev = 0.7\n",
    "\n",
    "start_stdevs_mean = 0\n",
    "start_stdevs_stdev = 1\n",
    "\n",
    "discount_factor = 0.9\n",
    "\n",
    "w_mean = np.random.normal(start_means_mean, start_means_stdev, size=(n_actions, n_state_features))\n",
    "w_stdev = np.absolute(np.random.normal(start_stdevs_mean, start_stdevs_stdev, size=(n_actions, n_state_features)))\n",
    "\n",
    "generation_size = 1000\n",
    "episodes_per_eval = 1\n",
    "proportion = 0.1\n",
    "n_candidates = math.floor(proportion * generation_size)\n",
    "\n",
    "k_noise = 0 # Noise when updating standard deviation\n",
    "\n",
    "rounds = 10\n",
    "\n",
    "delta_time = 1/60 # Time used for looking ahead to future states\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "print('Starting loop')\n",
    "for k in range(rounds):\n",
    "    # Sample agents from probability distribution\n",
    "    agents_unicycles = []\n",
    "\n",
    "    for _ in range(generation_size):\n",
    "        unicycle = Unicycle(position=track.generate_starting_position(), track=track)\n",
    "        w = np.random.normal(w_mean, w_stdev)\n",
    "        agent = GreedyLinearAgent(w, unicycle, delta_time)\n",
    "        agents_unicycles.append((agent, unicycle))\n",
    "\n",
    "    # Evaluate each agent, tracking the best-performing ones\n",
    "    candidates = []\n",
    "\n",
    "    for agent, unicycle in agents_unicycles:\n",
    "        # Play n episodes\n",
    "        average_return = 0\n",
    "        for i in range(episodes_per_eval):\n",
    "            delta_position = 0\n",
    "            iters_idle = 0\n",
    "            stuck = False\n",
    "            return_ = 0\n",
    "            \n",
    "            while True:\n",
    "                if unicycle.failed:\n",
    "                    break\n",
    "\n",
    "                old_x, old_y = unicycle.position\n",
    "                    \n",
    "                state = get_state(unicycle)\n",
    "                action = agent.policy(state)\n",
    "                reward = unicycle.step(action, delta_time)\n",
    "                \n",
    "                delta_position = math.hypot(unicycle.position[0] - old_x, unicycle.position[1] - old_y)\n",
    "\n",
    "                return_ = discount_factor * return_ + reward\n",
    "                \n",
    "                if delta_position < 1e-3:\n",
    "                    iters_idle += 1\n",
    "                else:\n",
    "                    iters_idle = 0\n",
    "                    \n",
    "                if iters_idle > 30:\n",
    "                    stuck = True\n",
    "                    break\n",
    "            \n",
    "            # Try to disincentivize getting stuck. (Try changing this?)\n",
    "            reward = -1 if stuck else score(state)\n",
    "            average_return += reward/episodes_per_eval\n",
    "\n",
    "        # If happened to do well, potentially add as candidate\n",
    "        if len(candidates) < n_candidates:\n",
    "            candidates.append((agent, reward))\n",
    "        else:\n",
    "            if reward > candidates[0][1]:\n",
    "                candidates[0] = (agent, reward)\n",
    "            candidates.sort(key=lambda ar: ar[1])\n",
    "\n",
    "    # Update new mean and stdev matrices, based on candidates and noise.\n",
    "    \n",
    "    new_w_stdev = 1/n_candidates * np.sum([np.abs(agent.w - w_mean) for agent, _ in candidates], axis=0)\n",
    "    new_w_stdev = np.abs(new_w_stdev + k_noise * np.random.randn(n_actions, n_state_features))\n",
    "    delta_w_stdev = np.linalg.norm(new_w_stdev - w_stdev)\n",
    "    w_stdev = new_w_stdev\n",
    "    \n",
    "    new_w_mean = 1/n_candidates * np.sum([agent.w for agent, _ in candidates], axis=0)\n",
    "    delta_w_mean = np.linalg.norm(new_w_mean - w_mean)\n",
    "    w_mean = new_w_mean\n",
    "    \n",
    "    best_agent_avg_reward = candidates[-1][1]\n",
    "    \n",
    "    xs.append(k)\n",
    "    ys.append(best_agent_avg_reward)\n",
    "    \n",
    "    for _, return_ in candidates:\n",
    "        print(return_)    \n",
    "\n",
    "    print(f'({k+1}/{rounds}) Finished evaluating generation. Best avg. reward: ', best_agent_avg_reward)\n",
    "    print('Updated distribution! Delta mean: ', delta_w_mean, '. Delta stdev: ', delta_w_stdev)\n",
    "\n",
    "# Return graph over each iteration\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a78c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea971de09964f32a8ba4ce35a8394b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(height=300, width=400)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n"
     ]
    }
   ],
   "source": [
    "canvas = Canvas(width=400, height=300)\n",
    "display(canvas)\n",
    "\n",
    "track = Loop(inner_radius, outer_radius)\n",
    "\n",
    "agent = candidates[0][0]\n",
    "unicycle = Unicycle(position=track.generate_starting_position(), track=track)\n",
    "\n",
    "n = 0\n",
    "while True:\n",
    "    with hold_canvas(): # wrap in hold_canvas() for performant canvas updates\n",
    "        if unicycle.failed: # .failed becomes true whenever unicycle gets off-track or tips over\n",
    "            break\n",
    "\n",
    "        state = get_state(unicycle)\n",
    "        action = agent.policy(state)\n",
    "        unicycle.step(action, delta_time)\n",
    "        \n",
    "        n += 1\n",
    "\n",
    "        draw(canvas, [track, unicycle])\n",
    "            \n",
    "        # Optional\n",
    "        sleep(delta_time)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9568e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/100) Finished epoch. Return: 6.795778835263244e-05\n",
      "(2/100) Finished epoch. Return: 0\n",
      "(3/100) Finished epoch. Return: 0\n",
      "(4/100) Finished epoch. Return: 0\n",
      "(5/100) Finished epoch. Return: 0\n",
      "(6/100) Finished epoch. Return: 0\n",
      "(7/100) Finished epoch. Return: 0\n",
      "(8/100) Finished epoch. Return: 0\n",
      "(9/100) Finished epoch. Return: 0\n",
      "(10/100) Finished epoch. Return: 0\n",
      "(11/100) Finished epoch. Return: 0\n",
      "(12/100) Finished epoch. Return: 0\n",
      "(13/100) Finished epoch. Return: 0\n",
      "(14/100) Finished epoch. Return: 0\n",
      "(15/100) Finished epoch. Return: 0\n",
      "(16/100) Finished epoch. Return: 0\n",
      "(17/100) Finished epoch. Return: 0\n",
      "(18/100) Finished epoch. Return: 0\n",
      "(19/100) Finished epoch. Return: 0\n",
      "(20/100) Finished epoch. Return: 0\n",
      "(21/100) Finished epoch. Return: 0\n",
      "(22/100) Finished epoch. Return: 0\n",
      "(23/100) Finished epoch. Return: 0\n",
      "(24/100) Finished epoch. Return: 0\n",
      "(25/100) Finished epoch. Return: 0\n",
      "(26/100) Finished epoch. Return: 0\n",
      "(27/100) Finished epoch. Return: 0\n",
      "(28/100) Finished epoch. Return: 0\n",
      "(29/100) Finished epoch. Return: 0\n",
      "(30/100) Finished epoch. Return: 0\n",
      "(31/100) Finished epoch. Return: 0\n",
      "(32/100) Finished epoch. Return: 0\n",
      "(33/100) Finished epoch. Return: 0\n",
      "(34/100) Finished epoch. Return: 0\n",
      "(35/100) Finished epoch. Return: 0\n",
      "(36/100) Finished epoch. Return: 0\n",
      "(37/100) Finished epoch. Return: 0\n",
      "(38/100) Finished epoch. Return: 0\n",
      "(39/100) Finished epoch. Return: 0\n",
      "(40/100) Finished epoch. Return: 0\n",
      "(41/100) Finished epoch. Return: 0\n",
      "(42/100) Finished epoch. Return: 0\n",
      "(43/100) Finished epoch. Return: 0\n",
      "(44/100) Finished epoch. Return: 0\n",
      "(45/100) Finished epoch. Return: 0\n",
      "(46/100) Finished epoch. Return: 0\n",
      "(47/100) Finished epoch. Return: 0\n",
      "(48/100) Finished epoch. Return: 0\n",
      "(49/100) Finished epoch. Return: 0\n",
      "(50/100) Finished epoch. Return: 0\n",
      "(51/100) Finished epoch. Return: 0\n",
      "(52/100) Finished epoch. Return: 0\n",
      "(53/100) Finished epoch. Return: 0\n",
      "(54/100) Finished epoch. Return: 0\n",
      "(55/100) Finished epoch. Return: 0\n",
      "(56/100) Finished epoch. Return: 0\n",
      "(57/100) Finished epoch. Return: 0\n",
      "(58/100) Finished epoch. Return: 0\n",
      "(59/100) Finished epoch. Return: 0\n",
      "(60/100) Finished epoch. Return: 0\n",
      "(61/100) Finished epoch. Return: 0\n",
      "(62/100) Finished epoch. Return: 0\n",
      "(63/100) Finished epoch. Return: 0\n",
      "(64/100) Finished epoch. Return: 0\n",
      "(65/100) Finished epoch. Return: 0\n",
      "(66/100) Finished epoch. Return: 0\n",
      "(67/100) Finished epoch. Return: 0\n",
      "(68/100) Finished epoch. Return: 0\n",
      "(69/100) Finished epoch. Return: 0\n",
      "(70/100) Finished epoch. Return: 0\n",
      "(71/100) Finished epoch. Return: 0\n",
      "(72/100) Finished epoch. Return: 0\n",
      "(73/100) Finished epoch. Return: 0\n",
      "(74/100) Finished epoch. Return: 0\n",
      "(75/100) Finished epoch. Return: 0\n",
      "(76/100) Finished epoch. Return: 0\n",
      "(77/100) Finished epoch. Return: 0\n",
      "(78/100) Finished epoch. Return: 0\n",
      "(79/100) Finished epoch. Return: 0\n",
      "(80/100) Finished epoch. Return: 0\n",
      "(81/100) Finished epoch. Return: 0\n",
      "(82/100) Finished epoch. Return: 0\n",
      "(83/100) Finished epoch. Return: 0\n",
      "(84/100) Finished epoch. Return: 0\n",
      "(85/100) Finished epoch. Return: 0\n",
      "(86/100) Finished epoch. Return: 0\n",
      "(87/100) Finished epoch. Return: 0\n",
      "(88/100) Finished epoch. Return: 0\n",
      "(89/100) Finished epoch. Return: 0\n",
      "(90/100) Finished epoch. Return: 0\n",
      "(91/100) Finished epoch. Return: 0\n",
      "(92/100) Finished epoch. Return: 0\n",
      "(93/100) Finished epoch. Return: 0\n",
      "(94/100) Finished epoch. Return: 0\n",
      "(95/100) Finished epoch. Return: 0\n",
      "(96/100) Finished epoch. Return: 0\n",
      "(97/100) Finished epoch. Return: 0\n",
      "(98/100) Finished epoch. Return: 0\n",
      "(99/100) Finished epoch. Return: 0\n",
      "(100/100) Finished epoch. Return: 0\n"
     ]
    }
   ],
   "source": [
    "# Q-Value Learning (in this case, through just a linear function, so technically not DQN).\n",
    "# Have one agent/policy. For each epoch, play some number of episodes,\n",
    "# occasionally collecting (randomly sampling) state/action/return tuples.\n",
    "# Then, at the end of the epoch, update the Q function through backups based on the tracked state-action pairs.\n",
    "\n",
    "from time import sleep\n",
    "from ipycanvas import Canvas, hold_canvas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "track = Loop(inner_radius, outer_radius)\n",
    "\n",
    "# Shape of input/output\n",
    "n_actions = 9\n",
    "n_state_features = 10\n",
    "\n",
    "# Hyperparameters for initial (meta-)weights generation\n",
    "epsilon = 0.95\n",
    "epsilon_decay = 0.995 # Get greedier over time\n",
    "episodes_per_epoch = 1\n",
    "memorize_p = 0.1\n",
    "discount_factor = 0.99\n",
    "\n",
    "rounds = 100\n",
    "\n",
    "delta_time = 1/60 # Time used for looking ahead to future states\n",
    "\n",
    "w = np.random.randn(n_actions, n_state_features)\n",
    "\n",
    "unicycle = Unicycle(position=track.generate_starting_position(), track=track)\n",
    "agent = EpsGreedyLinearAgent(w, unicycle, delta_time, eps=epsilon)\n",
    "\n",
    "for k in range(rounds):\n",
    "    memory = []\n",
    "\n",
    "    # Play n episodes\n",
    "    for i in range(episodes_per_epoch):\n",
    "        delta_position = 0\n",
    "        iters_idle = 0\n",
    "        stuck = False\n",
    "        \n",
    "        state_actions_to_replay = []\n",
    "\n",
    "        return_ = 0\n",
    "        \n",
    "        while True:\n",
    "            if unicycle.failed:\n",
    "                break\n",
    "\n",
    "            old_x, old_y = unicycle.position\n",
    "                \n",
    "            state = get_state(unicycle)\n",
    "            action = agent.policy(state)\n",
    "            \n",
    "            if np.random.randn() < memorize_p:\n",
    "                state_actions_to_replay.append((state, action))\n",
    "\n",
    "            reward = unicycle.step(action, delta_time)\n",
    "            \n",
    "            return_ = discount_factor * return_ + reward\n",
    "            \n",
    "            delta_position = math.hypot(unicycle.position[0] - old_x, unicycle.position[1] - old_y)\n",
    "            \n",
    "            if delta_position < 1e-3:\n",
    "                iters_idle += 1\n",
    "            else:\n",
    "                iters_idle = 0\n",
    "                \n",
    "            if iters_idle > 30:\n",
    "                stuck = True\n",
    "                return_ += -10\n",
    "                break\n",
    "        \n",
    "        for state, action in state_actions_to_replay:\n",
    "            memory.append((state, action, return_))\n",
    "\n",
    "    print(f'({k+1}/{rounds}) Finished epoch. Return: {return_}')\n",
    "\n",
    "    for state, action, return_ in memory:\n",
    "        agent.replay(state, action, return_) # Would normally pass subsequent state, but calculated from unicycle and action (deterministic).\n",
    "            \n",
    "    agent.eps *= epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a1a16e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
